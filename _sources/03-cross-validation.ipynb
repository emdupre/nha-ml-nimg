{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e56a9b7",
   "metadata": {},
   "source": [
    "# Evaluating our machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4395c7",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbea577",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nilearn import datasets\n",
    "\n",
    "os.environ[\"NILEARN_SHARED_DATA\"] = \"~/shared/data/nilearn_data\"\n",
    "datasets.get_data_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa5dd2",
   "metadata": {},
   "source": [
    "Now that we've run a few classifications models, what more could we need to know about machine learning in neuroimaging ?\n",
    "A lot, actually !\n",
    "Everything that we've done to date falls broadly under the umbrella of \"feature engineering.\"\n",
    "When applying machine learning, however, equally important are (1) the model that we train to generate predictions and (2) how we assess the generalizability of our learned model.\n",
    "\n",
    "In this notebook, we'll focus on (2).\n",
    "In particular, we'll highlight the importance of appropriate cross-validation methods.\n",
    "First, we can look at a recent review {cite}`Poldrack_2020` showing common cross-validation methods in neuroimaging:\n",
    "\n",
    "```{figure} ../images/poldrack-2020-fig3.jpg\n",
    "---\n",
    "height: 250px\n",
    "name: cv-usage\n",
    "---\n",
    "From {cite}`Poldrack_2020`, depicting results from a review of 100 studies (2017–2019) claiming prediction on fMRI data.\n",
    "_Panel A_ shows the prevalence of cross-validation methods in this sample.\n",
    "_Panel B_ shows a histogram of associated sample sizes.\n",
    "```\n",
    "\n",
    "As you can see, many neuroscience researchers are not using cross-validation at all !\n",
    "We will briefly overview _why_ cross-validation is so important to achieve, as well as different strategies for cross-validation that are in use with neuroimaging data.\n",
    "We then provide examples of appropriate and inappropriate cross-validation within the `development_fmri` dataset. \n",
    "One thing to emphasize, here : best practice is always to have a separate, held-out validation set !\n",
    "This will allow us to make more meaningful statements about our learned model,\n",
    "but it requires having access to more data.\n",
    "\n",
    "## Why cross-validate ?\n",
    "\n",
    "First, let's formalize the problem that cross-validation aims to solve, using notation from {cite}`Little_2017`. \n",
    "\n",
    "For $N$ observations, we can choose a variable $y \\in \\mathbb{R}^n$ that we are trying to predict from data $X \\in \\mathbb{R}^{n \\times p}$⁠.\n",
    "For example, we may have neuroimaging data for 155 participants, from which we are trying to predict their age group as either a child or an adult.\n",
    "\n",
    "The machine learning problem is to estimate a function $\\hat{f}_{\\{ train \\}}$ that predicts best $y$ from $X$.\n",
    "In other words, we want to minimize an error $\\mathcal{E}(y,\\hat{f}(X))$⁠.\n",
    "\n",
    "The challenge is that we are interested in this error on new, unknown, data.\n",
    "Thus, we would like to know the expectaction of the error for $(y, X)$ drawn from their unknown distribution:\n",
    "\n",
    "$$\n",
    "  \\mathbb{E}_{(y,X)} [\\mathcal{E}(y,\\hat{f}(X))].\n",
    "$$\n",
    "\n",
    "From this we note two important points.\n",
    "  1. Evaluation procedures _must_ test predictions of the model on held-out data that is independent from the data used to train the model.\n",
    "  2. Cross-validation procedures that repeating the train-test split many times to vary the training set also allow use to ask a related question:\n",
    "    given _future_ data to train a machine learning method on a clinical problem, what is the error that I can expect on new data?\n",
    "\n",
    "## Forms of cross-validation\n",
    "\n",
    "Given the importance of cross-validation in machine learning, many different schemes exist.\n",
    "The [scikit-learn documentation has a section](https://scikit-learn.org/stable/modules/cross_validation.html) just on this topic, which is worth reviewing in full.\n",
    "Here, we briefly highlight how cross-validation impacts our estimates in our example dataset.\n",
    "\n",
    "## Testing cross-validation schemes in our example dataset.\n",
    "\n",
    "We'll keep working with the same `development_dataset`, though this time we'll fetch all 155 subjects.\n",
    "Again, we'll derive functional connectivity matrices for each participant, though this time we'll only consider the \"correlation\" measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b533c1",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import (datasets, maskers, plotting)\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "development_dataset = datasets.fetch_development_fmri()\n",
    "msdl_atlas = datasets.fetch_atlas_msdl()\n",
    "\n",
    "masker = maskers.NiftiMapsMasker(\n",
    "    msdl_atlas.maps, resampling_target=\"data\",\n",
    "    t_r=2, detrend=True,\n",
    "    low_pass=0.1, high_pass=0.01).fit()\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "pooled_subjects = []\n",
    "groups = []  # child or adult\n",
    "\n",
    "for func_file, confound_file, (_, phenotypic) in zip(\n",
    "        development_dataset.func,\n",
    "        development_dataset.confounds,\n",
    "        development_dataset.phenotypic.iterrows()):\n",
    "\n",
    "    time_series = masker.transform(func_file, confounds=confound_file)\n",
    "    pooled_subjects.append(time_series)\n",
    "    groups.append(phenotypic['Child_Adult'])\n",
    "\n",
    "_, classes = np.unique(groups, return_inverse=True)\n",
    "pooled_subjects = np.asarray(pooled_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc5c17",
   "metadata": {},
   "source": [
    "In [our classification example](class-example), we used `StratifiedShuffleSplit` for cross-validation.\n",
    "This method preserves the percentage of samples for each class across train and test splits;\n",
    "that is, the percentages of child and adult participants in our classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# First, re-generate our cross-validation scores for StratifiedShuffleSplit\n",
    "\n",
    "strat_scores = []\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=30)\n",
    "for train, test in cv.split(pooled_subjects, groups):\n",
    "    connectivity = ConnectivityMeasure(kind=\"correlation\", vectorize=True)\n",
    "    connectomes = connectivity.fit_transform(pooled_subjects[train])\n",
    "    classifier = LinearSVC().fit(connectomes, classes[train])\n",
    "    predictions = classifier.predict(\n",
    "        connectivity.transform(pooled_subjects[test]))\n",
    "    strat_scores.append(accuracy_score(classes[test], predictions))\n",
    "\n",
    "print(f'StratifiedShuffleSplit Accuracy: {np.mean(strat_scores):.2f} ± {np.std(strat_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee178bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, generate a confusion matrix for the trained classifier\n",
    "# We'll plot just the last CV fold for now\n",
    "cm = ConfusionMatrixDisplay.from_predictions(classes[test], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753b966",
   "metadata": {},
   "source": [
    "What if we don't account for age groups when generating our cross-validation folds ?\n",
    "We can test this by using `KFold`, which does not stratify by group membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, compare with cross-validation scores for ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold_scores = []\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "for train, ktest in cv.split(pooled_subjects):\n",
    "    connectivity = ConnectivityMeasure(kind=\"correlation\", vectorize=True)\n",
    "    connectomes = connectivity.fit_transform(pooled_subjects[train])\n",
    "    classifier = LinearSVC().fit(connectomes, classes[train])\n",
    "    kfold_predictions = classifier.predict(\n",
    "        connectivity.transform(pooled_subjects[ktest]))\n",
    "    kfold_scores.append(accuracy_score(classes[ktest], kfold_predictions))\n",
    "\n",
    "print(f'KFold Accuracy: {np.mean(kfold_scores):.2f} ± {np.std(kfold_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3855fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, generate a confusion matrix for the trained classifier\n",
    "# We'll plot just the last CV fold for now\n",
    "cm = ConfusionMatrixDisplay.from_predictions(classes[ktest], kfold_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acbf1c",
   "metadata": {},
   "source": [
    "### Beyond accuracy: The Receiver-Operator Characteristic (ROC) Curve\n",
    "\n",
    "This exercise also shows the limitations of the _accuracy_ metric.\n",
    "It can be useful to look at other metrics, such as the Receiver-Operator Characteristic (ROC) Curve.\n",
    "Note that we're showing the ROC Curve for our StratifiedShuffleSplit model;\n",
    "our KFold model has an undefined area under the curve,\n",
    "since it only predicts one value !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    classes[test],\n",
    "    predictions,\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True,\n",
    ")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba76448",
   "metadata": {},
   "source": [
    "ROC curves can seem a bit harder to interpret than accuracy values,\n",
    "but we can think about them as defining our classifer's performance in a space of True Positives and False positives.\n",
    "\n",
    "```{figure} ../images/ROC_curve.png\n",
    "---\n",
    "height: 400px\n",
    "name: roc_curve\n",
    "---\n",
    "The ROC space for a \"better\" and \"worse\" classifier,\n",
    "from [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).\n",
    "```\n",
    "\n",
    "Our ROC curve also provides a useful visualization to look at the variability of our learned model across cross-validation folds !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e61795",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "cv = StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=30)\n",
    "\n",
    "for fold, (train, test) in enumerate(cv.split(pooled_subjects, groups)):\n",
    "    connectivity = ConnectivityMeasure(kind=\"correlation\", vectorize=True)\n",
    "    connectomes = connectivity.fit_transform(pooled_subjects[train])\n",
    "    classifier = LinearSVC().fit(connectomes, classes[train])\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        classifier,\n",
    "        connectivity.transform(pooled_subjects[test]),\n",
    "        classes[test],\n",
    "        name=f\"ROC fold {fold}\",\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "        plot_chance_level=(fold == 4),  # n_splits - 1\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=f\"Mean ROC (AUC = %0.2f ± %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"± 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6189797",
   "metadata": {},
   "source": [
    "## Small sample sizes give a wide distribution of errors\n",
    "\n",
    "Another common issue in cross-validation is when we only have access to small test set.\n",
    "\n",
    "```{figure} ../images/varoquaux-2017-fig1.png\n",
    "---\n",
    "height: 400px\n",
    "name: test-size\n",
    "---\n",
    "From {cite}`Varoquaux_2018`, this plot shows the distribution of errors between the prediction accuracy as assessed via cross-validation (average across folds) and as measured on a large independent test set for different types of neuroimaging data.\n",
    "Accuracy is reported for two reasonable choices of cross-validation strategy: leave-one-out (leave-one-run-out or leave-one-subject-out in data with multiple runs or subjects), or 50-times repeated splitting of 20% of the data.\n",
    "The bar and whiskers indicate the median and the 5th and 95th percentile. \n",
    "```\n",
    "\n",
    "The results show that these confidence bounds extends at least 10% both ways;\n",
    "that is, there is a 5% chance that it is 10% above the true generalization accuracy and a 5% chance this it is 10% below.\n",
    "This wide confidence bound is a result of an interaction between\n",
    "  1. the large sampling noise in neuroimaging data and\n",
    "  2. the relatively small sample sizes that we provide to the classifier.\n",
    "\n",
    "We can replicate this idea by systematically decreasing the size of our test set, first to 15 participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# med test set StratifiedShuffleSplit\n",
    "\n",
    "med_strat_scores = []\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=15)\n",
    "for train, test in cv.split(pooled_subjects, groups):\n",
    "    connectivity = ConnectivityMeasure(kind=\"correlation\", vectorize=True)\n",
    "    connectomes = connectivity.fit_transform(pooled_subjects[train])\n",
    "    classifier = LinearSVC().fit(connectomes, classes[train])\n",
    "    predictions = classifier.predict(\n",
    "        connectivity.transform(pooled_subjects[test]))\n",
    "    med_strat_scores.append(accuracy_score(classes[test], predictions))\n",
    "\n",
    "print(f'Medium StratifiedShuffleSplit Accuracy: {np.mean(med_strat_scores):.2f} ± {np.std(med_strat_scores):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cfd9a0",
   "metadata": {},
   "source": [
    "Then to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26506b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# small test set StratifiedShuffleSplit\n",
    "\n",
    "small_strat_scores = []\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=5)\n",
    "for train, test in cv.split(pooled_subjects, groups):\n",
    "    connectivity = ConnectivityMeasure(kind=\"correlation\", vectorize=True)\n",
    "    connectomes = connectivity.fit_transform(pooled_subjects[train])\n",
    "    classifier = LinearSVC().fit(connectomes, classes[train])\n",
    "    predictions = classifier.predict(\n",
    "        connectivity.transform(pooled_subjects[test]))\n",
    "    small_strat_scores.append(accuracy_score(classes[test], predictions))\n",
    "\n",
    "print(f'Small StratifiedShuffleSplit Accuracy: {np.mean(small_strat_scores):.2f} ± {np.std(small_strat_scores):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4f8b1",
   "metadata": {},
   "source": [
    "Then we can compare the distributions of these accuracy scores for each cross-validation scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "ax = sns.violinplot(\n",
    "    data=[strat_scores, med_strat_scores, small_strat_scores], \n",
    "    orient='h', \n",
    "    cut=0\n",
    ")\n",
    "ax.set(\n",
    "    yticklabels=[\n",
    "        'Stratified Shuffle (30)', \n",
    "        'Stratified Shuffle (15)', \n",
    "        'Stratified Shuffle (5)'\n",
    "    ],\n",
    "    ylabel='Cross-validation strategy',\n",
    "    xlabel='Accuracy'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad09fed",
   "metadata": {},
   "source": [
    "## Avoiding data leakage between train and test\n",
    "\n",
    "In {cite}`Varoquaux_2017`, Varoquaux and colleagues evaluated the impact of different cross-validation schemes on derived accuracy values.\n",
    "We reproduce their Figure 6 below.\n",
    "\n",
    "```{figure} ../images/varoquaux-2016-fig6.png\n",
    "---\n",
    "height: 400px\n",
    "name: cv-strategies\n",
    "---\n",
    "From {cite}`Varoquaux_2017` shows the difference in accuracy measured by cross-validation and on the held-out\n",
    "validation set, in intra and inter-subject settings, for different cross-validation strategies:\n",
    "(1) leave one sample out, (2) leave one block of samples out (where the block is the natural unit of the experiment: subject or session), and random splits leaving out 20% of the blocks as test data, with (3) 3, (4) 10, or (5) 50 random splits. \n",
    "For inter-subject settings, leave one sample out corresponds to leaving a session out.\n",
    "The box gives the quartiles, while the whiskers give the 5 and 95 percentiles.\n",
    "```\n",
    "\n",
    "We see that cross-validation schemes that \"leak\" information from the train to test set can give overly optimistic predictions.\n",
    "For example, if we leave-one-session-out for predictions within a participant, we see that our estimated prediction accuracy from cross-validation is much higher than our prediction accuracy on a held-out validation set.\n",
    "This is because different sessions from the same participant are highly-correlated;\n",
    "that is, participants are likely to show similar patterns of neural responses across sessions.\n",
    "\n",
    "In our dataset, this isn't a clear problem, since each participant was only sampled once.\n",
    "It is, though, something to stay aware of !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with cross-validation scores for leave-one-subject-out\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo_scores = []\n",
    "\n",
    "cv = LeaveOneOut()\n",
    "for train, test in cv.split(pooled_subjects):\n",
    "    connectivity = ConnectivityMeasure(kind=\"correlation\", vectorize=True)\n",
    "    connectomes = connectivity.fit_transform(pooled_subjects[train])\n",
    "    classifier = LinearSVC().fit(connectomes, classes[train])\n",
    "    predictions = classifier.predict(\n",
    "        connectivity.transform(pooled_subjects[test]))\n",
    "    loo_scores.append(accuracy_score(classes[test], predictions))\n",
    "\n",
    "print(f'Leave-One-Out Accuracy: {np.mean(loo_scores):.2f} ± {np.std(loo_scores):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37889c7d",
   "metadata": {},
   "source": [
    "```{bibliography} references.bib\n",
    ":style: unsrt\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   16,
   23,
   30,
   91,
   125,
   131,
   149,
   153,
   158,
   176,
   180,
   190,
   205,
   221,
   285,
   309,
   325,
   329,
   345,
   349,
   367,
   394,
   410
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}